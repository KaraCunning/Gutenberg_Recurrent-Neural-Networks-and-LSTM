
![gif](https://images.app.goo.gl/KgHjbycebMeufizH7)

![image](https://github.com/user-attachments/assets/bbee6c75-6a91-4a5d-8e70-8667886ffc12)


# Recurrent Neural Networks and Long Short Term Memory (LSTM)

## The Goal is to build a sequence model with the 
**Gutenberg Shakespeare Sonnet data**

### Model to predict the most likely next character and sonnet
Developed a deep learning model to generate Shakespearean-style sonnets using an LSTM-based sequence model.
Collected and preprocessed text data from Project Gutenberg, applying data cleaning techniques such as punctuation removal, case normalization, and sequence tokenization.
Utilized TensorFlow/Keras to build and train a Bidirectional LSTM model for next-character prediction.
Engineered training data by converting sonnets into character sequences and applying one-hot encoding.
Performed exploratory data analysis using Pandas, NumPy, and Matplotlib to determine optimal sequence lengths.
Improved model performance with embedding layers and dropout regularization to mitigate overfitting.
